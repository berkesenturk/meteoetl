version: '3.8'
services:
  postgres:
    image: postgres:16
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: meteoetl
    ports:
      - '5432:5432'
    volumes:
      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql

  airflow-init:
    image: apache/airflow:2.8.3-python3.11
    depends_on:
      - postgres
    entrypoint: /bin/bash
    environment:
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/meteoetl
      - PYTHONPATH=/opt/airflow/data_ingestion:/opt/airflow/data_processing:/opt/airflow/streaming

    command:
      - -c
      - |
        airflow db migrate && airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./data_ingestion:/opt/airflow/dags/data_ingestion
      - ./data_processing:/opt/airflow/dags/data_processing
      - ./streaming:/opt/airflow/dags/streaming
      - ./database:/opt/airflow/dags/database

  airflow-webserver:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    image: meteoetl-airflow:latest
    depends_on:
      - postgres
    environment:
      - LOAD_EX=n
      - EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/meteoetl
      - PYTHONPATH=/opt/airflow/data_ingestion:/opt/airflow/data_processing:/opt/airflow/streaming
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate && airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com || true
        exec airflow webserver
    ports:
      - "8080:8080"
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./data_ingestion:/opt/airflow/dags/data_ingestion
      - ./data_processing:/opt/airflow/dags/data_processing
      - ./streaming:/opt/airflow/dags/streaming
      - ./database:/opt/airflow/dags/database

  airflow-scheduler:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    image: meteoetl-airflow:latest
    depends_on:
      - postgres
    environment:
      - LOAD_EX=n
      - EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/meteoetl
      - PYTHONPATH=/opt/airflow/data_ingestion:/opt/airflow/data_processing:/opt/airflow/streaming
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate || true
        exec airflow scheduler
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./data_ingestion:/opt/airflow/dags/data_ingestion
      - ./data_processing:/opt/airflow/dags/data_processing
      - ./streaming:/opt/airflow/dags/streaming
      - ./database:/opt/airflow/dags/database

  grafana:
    image: grafana/grafana:10.2.1
    ports:
      - '3000:3000'
    depends_on:
      - postgres

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
    depends_on:
      - zookeeper

  spark-master:
    image: apache/spark:3.5.0
    container_name: spark-master
    hostname: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8083
    ports:
      - '8083:8083' # Master Web UI
      - '7077:7077' # Master Port
    volumes:
      - ./spark-apps:/opt/spark-apps
      - ./spark-data:/opt/spark-data

  spark-worker:
    image: apache/spark:3.5.0
    container_name: spark-worker
    hostname: spark-worker
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_WEBUI_PORT=8081
    ports:
      - '8081:8081' # Worker Web UI
    depends_on:
      - spark-master
    volumes:
      - ./spark-apps:/opt/spark-apps
      - ./spark-data:/opt/spark-data


  flink-jobmanager:
    image: apache/flink:1.17.2
    command: jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    ports:
      - "8082:8081"
    depends_on:
      - kafka

  flink-taskmanager:
    image: apache/flink:1.17.2
    command: taskmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    depends_on:
      - flink-jobmanager

